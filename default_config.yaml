{
  "train_mode": "fed", # fed(Federated), batch(Distributed mini-batch SGD), dec: Decentralized SGD
  "pipeline": 'default', # special tokens for algo specific iters

  "data_config":
    {
      "data_set": "mnist", # mnist, fashion-mnist, cifar10
      "shape": [28, 28],
      "num_labels": 10,
      "num_channels": 1,
      "download": False,

      "data_sampling_strategy": 'non_iid', # Data distribution (Heterogeneous / Non-Heterogeneous)
      "batch_size": 32, # Define Batch Size here since Data Loader will assign batches to clients
      "train_num_workers": 1,
    },


  "training_config":
    {
      "num_clients": 50, # Number of clients / parallel batches
      "client_fraction": 0.5, # specify fraction of client participation per round

      "global_epochs": 100, # Number of communication Rounds
      "local_epochs": 10, # Local SGD number of local steps

      "optimizer_config":
      {
        "optimizer": 'SGD', # SGD, Adam
        "loss": 'ce', # ce (Cross Entropy), mse (Mean Sq Error), bce(binary-CE)
        "lr0": 0.01,
        "momentum": 0,
        "reg": 0.0001,
        "nesterov": False,
        "amsgrad": False,

        "server_optimizer_config":
          {
            "optimizer": 'SGD',
            "lr0": 1,
          }
      },

      "lrs_config":
      {
        "lrs": 'step', # step, multi_step, exp, cyclic
        "milestones": [10, 25, 40],
        "step_size": 1,
        "gamma": 0.9
      },

      "compression_config":
      {
        "compression_operator": "full", # full (No Compression), top_k, rand_k
        "frac_coordinates_to_keep": 0.5, # For top-k, rand-k specify sparsity (k)
      },

      "learner_config":
      {
        "net": "mlp",
      },

      "aggregation_config":
      {
        "gar": "mean",
      },

    },
}