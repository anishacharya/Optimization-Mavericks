{
  "train_mode": "fed", # fed(Federated), distributed(Distributed mini-batch SGD), decentralized (Decentralized SGD)
  "pipeline": 'default', #

  "data_config":
    {
      "data_set": "mnist", # mnist, fashion-mnist, cifar10
      "shape": [28, 28],
      "num_labels": 10,
      "num_channels": 1,
      "download": False,

      "batch_size": 32, # Define Batch Size here since Data Loader will assign batches to clients
      "train_num_workers": 1,

      ## For Federated / Decentralized Training
      "data_sampling_strategy": 'iid', # Data distribution (Heterogeneous / Non-Heterogeneous)
      "num_shards": 100, # Number of shards for non-iid ~~ usually set it to k * num_of_clients 2 is a good value for k
    },


  "training_config":
    {
      "num_clients": 10, # Number of clients / parallel batches
      "client_fraction": 1, # specify fraction of client participation per round
      "global_epochs": 5, # Number of communication Rounds
      "local_epochs": 10, # Local SGD number of local steps

      "optimizer_config":
      {
        "optimizer": 'SGD', # SGD, Adam
        "loss": 'ce', # ce (Cross Entropy), mse (Mean Sq Error), bce(binary-CE)
        "lr0": 0.01,
        "momentum": 0.9,
        "reg": 0.0001,
        "nesterov": False,
        "amsgrad": False,

        "server_optimizer_config":
          {
            "optimizer": 'SGD',
            "lr0": 1,
          }
      },

      "lrs_config":
      {
        "lrs": 'step', # step, multi_step, exp, cyclic
        "milestones": [10, 25, 40],
        "step_size": 1,
        "gamma": 0.9
      },

      "compression_config":
      {
        "compression_operator": "full", # full (No Compression), top_k, rand_k
        "frac_coordinates_to_keep": 0.5, # For top-k, rand-k specify sparsity (k)
      },

      "learner_config":
      {
        "net": "mlp",
      },

      "aggregation_config":
      {
        "gar": "mean",
        "glomo_server_c": 1
      },

    },
}